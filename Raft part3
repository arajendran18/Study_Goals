### **Steps to Resolve `vault.raft.leader.lastcontact > 120` Alert** 

#### **1. Verify the Alert** 
- **Action**: Confirm if the alert is valid and not a false positive.  
  - Check if the condition persists in CloudWatch for multiple evaluation periods.  
  - Verify with Vault logs and other monitoring tools (e.g., Splunk, Grafana) to correlate the issue.  
- **Output**: Determine if the issue is ongoing or transient.  

---

#### **2. Check Vault Metrics** 
- **Metric**: Look at `vault.raft.leader.lastcontact` in CloudWatch or other monitoring systems.  
  - Check trends and identify when the latency started increasing.  
  - Look for correlated metrics such as:  
    - `vault.raft.replication.latency` (replication lag).  
    - `vault.raft.heartbeat_duration` (time taken for heartbeat communication).  
- **Output**: Identify spikes or abnormalities in Raft-related metrics.  

---

#### **3. Review Vault Logs** 
- **Action**: Examine logs for errors or warnings related to Raft consensus or leader communication.  
  - Common log messages to check:  
    - `"failed to contact followers"`  
    - `"Raft leader lost"`  
    - `"network timeout"`  
- **Tools**: Vault audit logs, syslogs, or Splunk (if integrated).  
- **Output**: Pinpoint specific issues, such as timeouts or resource contention.  

---

#### **4. Verify Node Connectivity** 
- **Action**: Ensure proper network connectivity between Raft cluster nodes.  
  - Check if all nodes can communicate without packet loss or high latency.  
  - Test using tools like `ping`, `traceroute`, or AWS VPC flow logs (if applicable).  
  - Verify DNS resolution and firewall rules between nodes.  
- **Output**: Confirm whether network issues are contributing to the delay.  

---

#### **5. Evaluate Resource Utilization** 
- **Action**: Check system resources on the leader and follower nodes.  
  - Metrics to monitor:  
    - **CPU**: Is the leader under high load?  
    - **Memory**: Are nodes running out of available memory?  
    - **Disk I/O**: Is slow disk performance causing replication delays?  
  - Tools: Use `top`, `iotop`, or CloudWatch instance metrics.  
- **Output**: Identify resource bottlenecks causing performance degradation.  

---

#### **6. Verify Leader Health** 
- **Action**: Ensure the leader node is stable and functioning correctly.  
  - Use `vault operator raft list-peers` to check the status of all cluster nodes.  
  - Confirm which node is the leader and validate its health.  
  - Investigate logs for leader-related errors.  
- **Output**: Determine if the leader is overburdened or misconfigured.  

---

#### **7. Inspect Raft Configuration** 
- **Action**: Validate and optimize the Raft cluster settings.  
  - Review parameters like:  
    - `raft_heartbeat_timeout` (e.g., reduce to `500ms`).  
    - `raft_election_timeout` (e.g., set to `1s`).  
  - **Example Configuration**:  
    ```hcl
    storage "raft" {
      path                  = "/opt/vault/data"
      raft_heartbeat_timeout = "500ms"
      raft_election_timeout  = "1s"
    }
    ```  
  - Restart Vault nodes after applying changes.  
- **Output**: Ensure that the configuration minimizes latency while maintaining stability.  

---

#### **8. Investigate Disk Performance**  
- **Action**: Check the disk performance of the leader node.  
  - Ensure sufficient disk throughput and low I/O latency.  
  - Look for logs indicating disk write issues or replication delays.  
- **Output**: Identify disk issues that may be causing replication delays.  

---

#### **9. Perform Cluster Validation**  
- **Action**: Run the following Vault commands to validate the cluster:  
  - `vault operator raft list-peers`: Check the status of each node.  
  - `vault operator raft step-down`: Force a new leader election (only if the leader is unresponsive).  
- **Output**: Ensure the cluster is operational and no nodes are stale.  

---

#### **10. Temporary Failover (If Necessary)**  
- **Action**: If the leader node is unstable:  
  - Use `vault operator raft step-down` to promote a new leader.  
  - Restart the affected node to rejoin the cluster.  
- **Output**: Stabilize the cluster by promoting a healthy leader.  

---

#### **11. Communicate Findings**  
- **Action**: Document and communicate:  
  - Root cause of the issue (e.g., network latency, disk I/O, leader instability).  
  - Steps taken to resolve the issue.  
  - Recommendations to prevent recurrence.  
- **Output**: Clear communication to stakeholders for incident resolution.  

---

By adding the **Raft Configuration Optimization** step, the SOP now includes a preventive and corrective measure to reduce consensus latency, improving the cluster's resilience and performance.
